---
layout:     post
title:      Pytorch 笔记
subtitle:  时刻进步
date:       2023-6-17
catalog: true
tags: [Python]

---

# PyTorch 笔记

> 学习过程中遇到的问题和解决方案

## 如何使用本地数据集

以`Cifar`数据集为例

- 先去官网下载好数据集

- 将数据集解压放到项目的`data`目录下（或者其他目录也行）

- 使用`torchvison`自带的`datasets`库来导入数据

  **不要更改解压后的目录名称，不然 datasets 库会找不到！**

  ```python
  import os
  from torch import nn
  import torch
  from torchvision import datasets, transforms
  import ConvAutoencoder
  #数据预处理
  transform = transforms.Compose([
      transforms.ToTensor()
  ])
  current_dir = os.getcwd()  # 获取当前工作目录
  parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))  # 获取当前工作目录的上一级目录
  data_path=os.path.join(parent_dir+"\data")
  
  train_data=datasets.CIFAR10(root=data_path,train=True,transform=transform,download=False)
  train_loader=torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, num_workers=2)
  ```

## 如何查看`DataLoader`中的数据形状

- 将对应的 loader 转换为迭代器，然后使用`next()`函数来从中取出一个数据

  ```python
  train_iter=iter(train_loader)
  image,label = next(train_iter)
  ```

- 接下来就可以查看数据形状了，直接`Jupyter notebook`中输出就完事了

  ```python
  image.shape
  ```

## 为什么 PyTorch 中，模型初始化的`super()`方法要带上模型自身的名称

例如：

```python
class CAE_decoder(nn.Module):  
    def __init__(self):
        super(CAE_decoder,self).__init__()
        self.conv1=nn.ConvTranspose2d(4,8,kernel_size=3,stride=1,padding=1)
        self.conv2=nn.ConvTranspose2d(8,16,kernel_size=3,stride=2,padding=1)
        self.conv3=nn.ConvTranspose2d(16,3,kernel_size=3,stride=2,padding=1)
        self.relu=nn.ReLU()

    def forward(self,x):  #x: [b,4,20,20]  -> [b,8,40,40]
        x=self.relu(self.conv1(x))  #[b,8,40,40]->[b,16,20,20
        x=self.relu(self.conv2(x))  #[b,16,20,20]->[b,8,10,10
        x=self.conv3(x)  #[b,8,10,10]->[b,3,5,
        return x
```

为什么要在 `super()` 中带上类自身的名称 `CAE_decoder` 呢？

这是因为在多重继承的情况下，一个类可能同时继承自多个父类，因此需要指定具体要调用的父类。在单一继承的情况下，指定类自身的名称并不是必需的，可以简化为 `super().__init__()`。

但是为了保持代码的清晰和明确，推荐在 `super()` 中明确指定类自身的名称，以便在代码中明确指定调用的父类方法。

总结起来，`super(CAE_decoder, self).__init__()` 的作用是调用 `CAE_decoder` 类的父类 `nn.Module` 的初始化方法，以初始化 `CAE_decoder` 类的对象。

## 为什么 PyTorch 中数组里面放向量可以获得索引值

在学习D2L的时候看到这样一段代码

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

很不理解其中的`features[batch_indices]`语句，然后在 Python 里面试了一下，写了以下代码

```python
a={"Google","Alibaba","Baidu","Tencent"}
c=list(a)
d=list(range(0,2))
print(c[d])
```

我就很奇怪，一直报错

```bash
TypeError: list indices must be integers or slices, not list
```

但是在第一段代码里面就是可以啊，最终查找资料发现，**在一个数组里面放一个列表，获得对应索引值所指向的元素，是`torch`中`tensor`的特性，在 Python 的`list`中不可以这么用**

## 在多次运行 Python 脚本时如何防止内存累计

Python 解释器在 Windows平台 执行创建多进程的程序时，子进程会读取当前 Python 文件，用以创建进程，在子进程读取当前文件时，读取到创建子进程的代码时又会创建新的子进程，这样程序就陷入递归创建进程的状态

由于 Python 解释器中对于创建子进程有一个最大数量限制来保护我们的计算机，防止其内存溢出，因此程序会抛出异常

```bash
RuntimeError:
An attempt has been made to start a new process before the
current process has finished its bootstrapping phase.
This probably means that you are not using fork to start your
child processes and you have forgotten to use the proper idiom
in the main module:
　　if name == ‘main‘:
　　　　freeze_support()
　　　　…
The “freeze_support()” line can be omitted if the program
is not going to be frozen to produce an executable.
```

这样的话我们就需要在脚本中使用以下方式来建立程序入口

```python
if __name__ == '__main__':
    main()
```

但这样又会产生一个问题，在当前脚本执行结束后，该脚本的进程并不会自动终止，而是会一直运行，这样下去，会占用越来越多的系统的内存

解决方式，加上一行代码，在脚本执行完毕后自动终止当前 Python 解释器

```python
if __name__ == '__main__':
    main()
    sys.exit()
```

## 如何对网络中每一层的权重进行初始化操作

如果我们将继承自`nn.Module`的网络直接传入`for...in... `语句，第一个元素会是网络自身，因此，初始化的示例函数如下：

```python
def init_weight(net):
    for layers in net.modules():
        if(type(layers) == nn.Conv2d or type(layers) == nn.Linear):
            nn.init.xavier_uniform_(layers.weight)
```







# 一些概念

## `镜像填充（mirror-pad）`

在深度学习中，"mirror-pad"是一种填充（padding）技术，用于在图像或特征图的周围添加边缘像素值。它也被称为"反射填充"或"镜像填充"。

在卷积神经网络（CNN）等深度学习模型中，常常需要对输入数据进行填充以保持特征图的尺寸。填充可以用于多种目的，例如防止特征图在卷积操作中缩小过快，使得特征提取更为准确。

与常规填充方式（如全零填充）不同，mirror-pad使用了镜像对称的方式填充边缘像素。具体来说，对于输入图像或特征图的边缘像素，mirror-pad会在其周围进行镜像复制。这样一来，填充后的边缘像素与原始图像的边缘更加平滑，能够更好地保留图像的结构信息。

下图展示了一个示例，说明了mirror-pad的效果：

```bash
Copy code原始图像：
1 2 3
4 5 6
7 8 9

mirror-pad后的图像：
9 8 7 8 9
6 5 4 5 6
3 2 1 2 3
6 5 4 5 6
9 8 7 8 9
```

## 熵模型

熵模型指的是用来描述压缩图像时潜在表征（latent representation）的熵的模型。熵模型通常采用参数化形式的估计，参数根据数据进行拟合。与潜在表征的边缘分布（marginal distribution）相区分，熵模型是未知分布，它同时受到被编码图像的分布和用于推断替代表征的方法的影响。一个压缩系统利用熵模型来压缩潜在表征，最小化期望码长度，即压缩表示的平均长度，并与原始图像之间的期望差异。

## 熵模型中的熵

熵指的是用于压缩潜在表示的熵模型中的熵。熵是一个表示信息量的度量，**表示了在概率分布中获得新信息的平均代价**。在压缩过程中，使用熵模型来对**`潜在表示`**进行压缩，以达到最小化期望编码长度的目标。熵模型通常被假设为具有某种参数化形式的估计分布，该分布的参数是根据数据拟合得到的。熵的最小化意味着熵模型的分布与潜在表示的实际边缘分布相匹配，从而提高了压缩性能。

## **算术编码器和算数解码器**

> 下图中的Q代表量化，AE代表算术编码器，AD代表算术解码器

**算术编码器**和**算数解码器**在图像压缩中起到了重要作用。算术编码器用于将离散值的压缩信号转换为位流，而算术解码器则将位流转换回压缩信号，实现对图像的重建。在这个过程中，算术编码器旨在通过有效地编码压缩信号的概率分布来最小化压缩后的编码长度，而算术解码器则通过解码位流并利用概率分布来恢复原始的压缩信号。

具体来说，算术编码器通过统计压缩信号中不同值的出现频率来确定它们的概率分布。然后，它使用这些概率来动态地编码每个值，其中出现频率高的值用较短的编码表示，而出现频率低的值则用较长的编码表示。这样，算术编码器可以将整个压缩信号编码为一个紧凑的位流。

相反，算术解码器接收到位流后，利用相同的概率分布来解码每个位，从而逐步恢复压缩信号。通过将每个位与概率分布相结合，算术解码器可以确定最可能的原始数值，并逐步恢复整个压缩信号。

总的来说，算术编码器和算术解码器是图像压缩中用于编码和解码压缩信号的关键组件，并且通过有效地利用概率分布来实现高效的图像压缩和重建。

![image-20230702232946176](https://s2.loli.net/2023/07/02/sA1lLCFzhYBrnSc.png)

# 常见错误

## 在网络定义中，对最后一层应用了激活函数

实际上不行

## 激活函数未定义就直接使用了

例如：

```python
nn.ReLU(self.conv1(x)) 
```

实际上这是错的，上面这行进行的是类的初始化操作，传入一个 Tensor 肯定就错了

应该这样：

```python
class CAE_decoder(nn.Module):  
    def __init__(self):
        super(CAE_decoder,self).__init__()
        self.conv1=nn.ConvTranspose2d(4,8,kernel_size=3,stride=1,padding=1)
        self.conv2=nn.ConvTranspose2d(8,16,kernel_size=3,stride=2,padding=1)
        self.conv3=nn.ConvTranspose2d(16,3,kernel_size=3,stride=2,padding=1)
        self.relu=nn.ReLU()

    def forward(self,x):  #x: [b,4,20,20]  -> [b,8,40,40]
        x=self.relu(self.conv1(x))  #[b,8,40,40]->[b,16,20,20
        x=self.relu(self.conv2(x))  #[b,16,20,20]->[b,8,10,10
        x=self.conv3(x)  #[b,8,10,10]->[b,3,5,
        return x
```

