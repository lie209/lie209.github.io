---
layout: post
title: 深度学习图像压缩
date: 2023-7-13
catalog: true
tags: [论文]
---

# 深度学习图像压缩笔记

## 镜像填充（mirror-pad）

在深度学习中，"mirror-pad"是一种填充（padding）技术，用于在图像或特征图的周围添加边缘像素值。它也被称为"反射填充"或"镜像填充"。

在卷积神经网络（CNN）等深度学习模型中，常常需要对输入数据进行填充以保持特征图的尺寸。填充可以用于多种目的，例如防止特征图在卷积操作中缩小过快，使得特征提取更为准确。

与常规填充方式（如全零填充）不同，mirror-pad使用了镜像对称的方式填充边缘像素。具体来说，对于输入图像或特征图的边缘像素，mirror-pad会在其周围进行镜像复制。这样一来，填充后的边缘像素与原始图像的边缘更加平滑，能够更好地保留图像的结构信息。

下图展示了一个示例，说明了mirror-pad的效果：

```bash
Copy code原始图像：
1 2 3
4 5 6
7 8 9

mirror-pad后的图像：
9 8 7 8 9
6 5 4 5 6
3 2 1 2 3
6 5 4 5 6
9 8 7 8 9
```

## 熵估计模型

熵估计模型在深度学习图像压缩中的作用是用来估计压缩后图像数据的信息熵，从而实现对图像数据的压缩编码。它用于建模图像数据的统计特性和概率分布，以便更好地进行压缩编码和解码过程。熵估计模型能够根据输入的图像数据提供相应的概率分布信息，以及用于计算压缩率和失真度量的比特率和失真参数。通过熵估计模型，我们能够评估压缩编码过程中需要的比特数，实现在保持图像质量的同时尽量减少所需的存储空间或传输带宽。在文档中[2a]提到了当前深度学习图像压缩方法中常见的熵估计模型设计方法，如引入额外的隐变量作为超先验，通过引入一些上下文信息来提高熵模型的性能。此外，文档还提到了[2b]最近一些使用注意力机制的熵估计模型，通过注意力模块的应用来提高模型的设计和性能。

因此，熵估计模型在深度学习图像压缩中的作用是对图像数据进行建模和概率估计，用于度量压缩编码的效率和图像质量，并在压缩和解码过程中实现数据的高效传输和存储。

## 熵模型中的熵

熵指的是用于压缩潜在表示的熵模型中的熵。熵是一个表示信息量的度量，**表示了在概率分布中获得新信息的平均代价**。在压缩过程中，使用熵模型来对**`潜在表示`**进行压缩，以达到最小化期望编码长度的目标。熵模型通常被假设为具有某种参数化形式的估计分布，该分布的参数是根据数据拟合得到的。熵的最小化意味着熵模型的分布与潜在表示的实际边缘分布相匹配，从而提高了压缩性能。

## **算术编码器和算数解码器**

> 下图中的Q代表量化，AE代表算术编码器，AD代表算术解码器

**算术编码器**和**算数解码器**在图像压缩中起到了重要作用。算术编码器用于将离散值的压缩信号转换为位流，而算术解码器则将位流转换回压缩信号，实现对图像的重建。在这个过程中，算术编码器旨在通过有效地编码压缩信号的概率分布来最小化压缩后的编码长度，而算术解码器则通过解码位流并利用概率分布来恢复原始的压缩信号。

具体来说，算术编码器通过统计压缩信号中不同值的出现频率来确定它们的概率分布。然后，它使用这些概率来动态地编码每个值，其中出现频率高的值用较短的编码表示，而出现频率低的值则用较长的编码表示。这样，算术编码器可以将整个压缩信号编码为一个紧凑的位流。

相反，算术解码器接收到位流后，利用相同的概率分布来解码每个位，从而逐步恢复压缩信号。通过将每个位与概率分布相结合，算术解码器可以确定最可能的原始数值，并逐步恢复整个压缩信号。

总的来说，算术编码器和算术解码器是图像压缩中用于编码和解码压缩信号的关键组件，并且通过有效地利用概率分布来实现高效的图像压缩和重建。

![image-20230702232946176](https://s2.loli.net/2023/07/02/sA1lLCFzhYBrnSc.png)

## 具有熵模型的深度学习图像压缩网络的Pytorch简易实现

```python
import torch
import torch.nn as nn
from compressai.entropy_models import EntropyBottleneck

class CompressionNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        
        # 编码网络
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 5, stride=2),
            nn.ReLU(),
            nn.Conv2d(32, 64, 5, stride=2),
            nn.ReLU(),
        )
        
        # 量化层
        self.quantizer = nn.Sequential(
            nn.Conv2d(64, 32, 5, stride=2),
            nn.ReLU(),
        )
        
        # 熵模型
        self.entropy_bottleneck = EntropyBottleneck(32)

        # 解码网络
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(32, 64, 5, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 5, stride=2),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 5, stride=2),
        )
        
    def forward(self, x):
        # 编码
        y = self.encoder(x)
        
        # 量化
        z = self.quantizer(y)
        
        # 熵编码
        z_hat, z_likelihoods = self.entropy_bottleneck(z)
        
        # 解码
        y_hat = self.decoder(z_hat)
        
        return y_hat, z_likelihoods
```

这个网络包含:

- 编码网络:使用卷积层提取图像特征
- 量化层:将编码特征进一步量化
- 熵模型:使用可微分的EntropyBottleneck对量化特征进行无损压缩
- 解码网络:解码压缩后的特征并重建图像

在训练时,我们可以 jointly 优化重建质量和熵编码长度,实现端到端的深度压缩

这是一个简单的示例,可以继续扩展网络结构和训练过程来获得更好的压缩效果

## 直通估计器

直通估计器(straight-through estimator)是一种用于训练含有不可导操作(如量化、采样等)的网络的技巧。

其主要思想是:

1. 在前向传播时,执行不可导操作,得到输出。
2. 在反向传播时,不传导该不可导操作的梯度,而是直接传导操作输入的梯度。

以量化为例:

前向:output = quantize(input)

反向: dLoss/dinput = dLoss/doutput

这样可以避免不可导操作对训练过程的影响,使网络仍然可导,可以通过梯度下降进行优化。

直通估计让训练过程可以优化整个压缩重构流程,而不是只优化量化前的部分,从而获得更好的模型。这是一种常见的训练含量化网络的技巧。

## 量化误差

量化操作Q不可避免地会引入量化误差(clipping error),导致重建图像产生歪变。

具体来说:

1. 在编码器的输出端,需要对特征latent y进行量化,得到量化后的表示ŷ。
2. 量化操作Q是一种不可逆过程,它通过离散化将y映射到离散的代表值ŷ。
3. 这个离散化过程会造成量化误差,即y和ŷ之间存在差异: error = y - Q(y)。
4. 这种量化引入的误差是一种截断误差(clipping error),它截断了y中代表ŷ无法表示的信息。
5. 在解码端,量化后的ŷ作为输入,相比原始的y损失了部分信息。
6. 解码器基于ŷ重建图像,这时就会由于信息的损失而出现重建图像质量下降的问题。
7. 量化误差的存在,导致了重建图像与原图之间的歪变(distortion)。

所以,量化操作不可避免地会损失一些信息,引入误差,这种误差会积累并导致解码图像质量的下降。这是一种压缩中常见的量化误差影响。

## 切片操作

图像压缩模型中进行切片操作的主要目的是:

1. 将高维的特征表示分割成多份,便于并行处理,降低计算复杂度。
2. 切片后,每片上的条件熵编码可以独立并行地进行。
3. 在压缩某一片时,可以利用先前片的信息作为条件概率的先验。
4. 通过轮次回归,可以迭代地优化每片的压缩结果。
5. 分片处理还可以加强压缩的鲁棒性,如果某片数据丢失,其他片可以继续重建。
6. 合理划分片区,可以使每个片包含相对局部的特征信息。
7. 相比整体联合编码,分片独立编码可以减小计算内存开销。
8. 利用切片可以方便地进行分块传输,适合流传输场景。
9. 一定程度上也提供了可扩展性和可调整性。

所以,利用切片技术,可以有效实现**并行编码、轮次处理、增强鲁棒性,并降低模型的计算和内存复杂度**,这是进行切片的主要考虑

## EntropyBottleneck 的 CDF 参数

在图像压缩框架中,CDF(Cumulative Distribution Function, 累积分布函数)是实现熵编码的关键参数。

具体来说:

1. 熵编码基于概率分布对数据进行压缩,压缩率与概率分布的准确性直接相关。
2. 对于要压缩的数据(这里是图像的latent表示),会首先估计它的概率分布,比如高斯分布。
3. 根据这个概率分布函数,可以计算出CDF,也就是每个取值的累积概率。
4. 在压缩时,用CDF将数据值转换成概率值,再转化为码字进行熵编码。
5. 在解码时,直接用CDF的逆过程根据码字反推回数据值。
6. 所以CDF参数反映了数据的概率分布信息,是实现无损熵编码的关键。
7. 在图像压缩中,需要训练网络来学习更新CDF参数,以更准确地适应图像数据的分布。
8. 这样可以减小压缩误差,提高压缩效果。

所以CDF参数对于熵编码在图像压缩中的应用非常重要,需要特别处理来实现模型加载和更新。